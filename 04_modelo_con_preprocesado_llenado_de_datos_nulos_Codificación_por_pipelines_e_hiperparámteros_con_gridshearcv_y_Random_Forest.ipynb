{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguel-173/Proyecto_1/blob/main/04_modelo_con_preprocesado_llenado_de_datos_nulos_Codificaci%C3%B3n_por_pipelines_e_hiperpar%C3%A1mteros_con_gridshearcv_y_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PROYECTO IA 2024-1**\n",
        "\n",
        "\n",
        "##Carga de datos\n"
      ],
      "metadata": {
        "id": "untZG1k6ITJD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brqMXD8hbkAf"
      },
      "outputs": [],
      "source": [
        "#importar librerías para lectura Kaggle\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \".\" #ubicación"
      ],
      "metadata": {
        "id": "xG1xx0RVeFus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c udea-ai4eng-20241 #acceso a la información del Kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfelLjahc965",
        "outputId": "1c905eae-8ab6-4330-87db-58108d69970a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n",
            "udea-ai4eng-20241.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip udea-ai4eng-20241.zip #descargar zip con data CSV"
      ],
      "metadata": {
        "id": "xAn4-_2oeu6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\") #lectura de datos\n"
      ],
      "metadata": {
        "id": "LkbMTbYOgr5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 04 - Modelo con preprocesado llenado de datos nulos. Codificación por pipelines e hiperparámteros con gridshearcv\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b2znOeEzQj21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "target_column = 'target'\n",
        "\n",
        "# Separar características y objetivo\n",
        "X = df.drop(columns=[target_column])\n",
        "y = df[target_column]\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Separar características numéricas y categóricas\n",
        "caracteristicas_numericas = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "caracteristicas_categoricas = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Pipelines para transformación de datos\n",
        "pipeline_numerico = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Asegura la imputación para evitar errores con el scaler\n",
        "    ('scaler', MinMaxScaler())  # Cambia a StandardScaler() si prefieres estandarizar\n",
        "])\n",
        "\n",
        "pipeline_categorico = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputación de valores faltantes en categóricas\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', pipeline_numerico, caracteristicas_numericas),\n",
        "    ('cat', pipeline_categorico, caracteristicas_categoricas)\n",
        "])\n",
        "\n",
        "# Crear un pipeline para el modelo completo\n",
        "model_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Definir el grid de hiperparámetros\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [None, 10, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'classifier__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "# Entrenar el modelo con GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtener los mejores parámetros y el mejor estimador\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"Mejores hiperparámetros:\", best_params)\n",
        "\n",
        "# Hacer predicciones con el mejor modelo\n",
        "y_pred = best_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "sp8d2NAU2Q7M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}